{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbfcb64-1e39-49c9-8d20-235a68bb64ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Paused\n",
      "System Resumed\n",
      "System Paused\n",
      "System Resumed\n",
      "System Paused\n",
      "System Resumed\n",
      "System Paused\n",
      "System Resumed\n",
      "System Paused\n",
      "System Resumed\n",
      "System Paused\n",
      "System Resumed\n",
      "System Paused\n",
      "System Resumed\n",
      "System Paused\n",
      "System Resumed\n",
      "System Paused\n",
      "System Resumed\n",
      "System Paused\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "\n",
    "# Disable PyAutoGUI fail-safe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Initialize camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize cursor position\n",
    "cursor_x, cursor_y = 0, 0\n",
    "last_cursor_x, last_cursor_y = 0, 0  # Store last cursor position\n",
    "smoothing_factor = 0.7  # Adjust this value for more or less smoothing\n",
    "sensitivity_multiplier = 5.0  # Increased sensitivity for better navigation\n",
    "\n",
    "# Initialize pause state\n",
    "is_paused = False\n",
    "\n",
    "# Function to detect swipe gestures\n",
    "def detect_gesture(hand_landmarks, handedness):\n",
    "    # Get the positions of the index, middle, and thumb fingers\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
    "\n",
    "    # Calculate the distance between the tips of the fingers\n",
    "    distance = np.linalg.norm(np.array([index_finger_tip.x, index_finger_tip.y]) - np.array([middle_finger_tip.x, middle_finger_tip.y]))\n",
    "    pinch_distance = np.linalg.norm(np.array([index_finger_tip.x, index_finger_tip.y]) - np.array([thumb_tip.x, thumb_tip.y]))\n",
    "\n",
    "    # Detect scrolling based on left middle finger and thumb coming together\n",
    "    if handedness == \"Left\":\n",
    "        if (np.linalg.norm(np.array([middle_finger_tip.x, middle_finger_tip.y]) - np.array([thumb_tip.x, thumb_tip.y])) < 0.05):\n",
    "            return \"scroll_up\"  # Scroll up when middle finger and thumb come together\n",
    "        elif (np.linalg.norm(np.array([ring_finger_tip.x, ring_finger_tip.y]) - np.array([thumb_tip.x, thumb_tip.y])) < 0.05):\n",
    "            return \"scroll_down\"  # Scroll down when ring finger and thumb come together\n",
    "\n",
    "    # Detect pinch gesture using thumb and index finger\n",
    "    if pinch_distance < 0.05:  # Pinch detected\n",
    "        return \"pinch\"\n",
    "    \n",
    "    # Detect zoom out gesture using pinky and thumb\n",
    "    if handedness == \"Left\" and (np.linalg.norm(np.array([pinky_tip.x, pinky_tip.y]) - np.array([thumb_tip.x, thumb_tip.y])) < 0.05):\n",
    "        return \"zoom_out\"  # Zoom out when pinky and thumb come together\n",
    "\n",
    "    # Detect zoom in gesture using right pinky and thumb\n",
    "    if handedness == \"Right\" and (np.linalg.norm(np.array([pinky_tip.x, pinky_tip.y]) - np.array([thumb_tip.x, thumb_tip.y])) < 0.05):\n",
    "        return \"zoom_in\"  # Zoom in when right pinky and thumb come together\n",
    "\n",
    "    # Detect volume up gesture using right thumb and middle finger\n",
    "    if handedness == \"Right\":\n",
    "        if (np.linalg.norm(np.array([thumb_tip.x, thumb_tip.y]) - np.array([middle_finger_tip.x, middle_finger_tip.y])) < 0.05):\n",
    "            return \"volume_up\"  # Volume up when thumb and middle finger come together\n",
    "        # Detect volume down gesture using right ring finger and thumb\n",
    "        if (np.linalg.norm(np.array([thumb_tip.x, thumb_tip.y]) - np.array([ring_finger_tip.x, ring_finger_tip.y])) < 0.05):\n",
    "            return \"volume_down\"  # Volume down when thumb and ring finger come together\n",
    "    \n",
    "    # Detect mute gesture using thumb, middle finger, and ring finger\n",
    "    if handedness == \"Right\":\n",
    "        if (np.linalg.norm(np.array([middle_finger_tip.x, middle_finger_tip.y]) - np.array([thumb_tip.x, thumb_tip.y])) < 0.05 and\n",
    "            np.linalg.norm(np.array([ring_finger_tip.x, ring_finger_tip.y]) - np.array([thumb_tip.x, thumb_tip.y])) < 0.05):\n",
    "            return \"mute\"  # Mute when thumb, middle finger, and ring finger come together\n",
    "\n",
    "    # Detect fully fisted hand\n",
    "    if all([landmark.visibility > 0.5 for landmark in hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP:mp_hands.HandLandmark.PINKY_TIP + 1]]):\n",
    "        return \"fist\"\n",
    "\n",
    "def main():\n",
    "    global cursor_x, cursor_y, last_cursor_x, last_cursor_y, is_paused\n",
    "    left_clicking = False\n",
    "    right_clicking = False\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Flip the frame horizontally for a later selfie-view display\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the BGR image to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame and find hands only if not paused\n",
    "        if not is_paused:\n",
    "            results = hands.process(rgb_frame)\n",
    "\n",
    "            # Draw hand landmarks and detect gestures\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:  # Process all detected hands\n",
    "                    mp.solutions.drawing_utils.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                    \n",
    "                    # Determine if the hand is left or right\n",
    "                    handedness = results.multi_handedness[results.multi_hand_landmarks.index(hand_landmarks)].classification[0].label\n",
    "                    \n",
    "                    # Detect gestures based on the hand\n",
    "                    gesture = detect_gesture(hand_landmarks, handedness)\n",
    "\n",
    "                    # Bind mouse cursor movement to the right hand\n",
    "                    if handedness == \"Right\":\n",
    "                        # Calculate the average position of all fingertips\n",
    "                        fingertips = [\n",
    "                            hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP],\n",
    "                            hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP],\n",
    "                            hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP],\n",
    "                            hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP],\n",
    "                            hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
    "                        ]\n",
    "                        avg_x = sum([finger.x for finger in fingertips]) / len(fingertips)\n",
    "                        avg_y = sum([finger.y for finger in fingertips]) / len(fingertips)\n",
    "\n",
    "                        cursor_x = int((avg_x * frame.shape[1]) * sensitivity_multiplier)\n",
    "                        cursor_y = int((avg_y * frame.shape[0]) * sensitivity_multiplier)\n",
    "\n",
    "                        # Smoothing for stable movement\n",
    "                        cursor_x = int(cursor_x * smoothing_factor + last_cursor_x * (1 - smoothing_factor))\n",
    "                        cursor_y = int(cursor_y * smoothing_factor + last_cursor_y * (1 - smoothing_factor))\n",
    "                        \n",
    "                        # Move the mouse cursor to the position of the average fingertip\n",
    "                        pyautogui.moveTo(cursor_x, cursor_y)\n",
    "\n",
    "                        # Update last cursor position\n",
    "                        last_cursor_x, last_cursor_y = cursor_x, cursor_y\n",
    "\n",
    "                        # Handle right pinch for right click\n",
    "                        if gesture == \"pinch\":\n",
    "                            pyautogui.click(button='right')  # Simulate right click\n",
    "\n",
    "                    # Center cursor when palm is detected\n",
    "                    if gesture == \"palm\":\n",
    "                        screen_center_x = int(frame.shape[1] / 2)\n",
    "                        screen_center_y = int(frame.shape[0] / 2)\n",
    "                        pyautogui.moveTo(screen_center_x, screen_center_y)\n",
    "\n",
    "                    # Display gesture information on the frame\n",
    "                    cv2.putText(frame, f\"Gesture: {gesture}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 120, 0), 2)\n",
    "\n",
    "                    # Handle left hand gestures\n",
    "                    if handedness == \"Left\":\n",
    "                        if gesture == \"scroll_up\":\n",
    "                            pyautogui.scroll(10)  # Scroll up\n",
    "                        elif gesture == \"scroll_down\":\n",
    "                            pyautogui.scroll(-10)  # Scroll down\n",
    "                        elif gesture == \"pinch\":\n",
    "                            pyautogui.click()  # Simulate left click\n",
    "                        elif gesture == \"zoom_out\":\n",
    "                            pyautogui.hotkey('ctrl', '-')  # Zoom out\n",
    "\n",
    "                    # Handle right hand gestures\n",
    "                    if handedness == \"Right\":\n",
    "                        if gesture == \"zoom_in\": \n",
    "                            pyautogui.hotkey('ctrl', '+')  # Zoom in\n",
    "                        elif gesture == \"volume_up\":\n",
    "                            pyautogui.hotkey('volumeup')  # Simulate volume up\n",
    "                        elif gesture == \"volume_down\":\n",
    "                            pyautogui.hotkey('volumedown')  # Simulate volume down\n",
    "                        elif gesture == \"mute\":\n",
    "                            pyautogui.hotkey('volumemute')  # Simulate mute\n",
    "\n",
    "        # Display pause status on frame\n",
    "        status_text = \"PAUSED\" if is_paused else \"ACTIVE\"\n",
    "        status_color = (0, 0, 255) if is_paused else (0, 255, 0)  # Red when paused, Green when active\n",
    "        cv2.putText(frame, f\"Status: {status_text}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, status_color, 2)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Gesture Control', frame)\n",
    "\n",
    "        # Handle keyboard input\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('p'):  # Toggle pause state when 'p' is pressed\n",
    "            is_paused = not is_paused\n",
    "            print(f\"System {'Paused' if is_paused else 'Resumed'}\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d61b6-0e38-4f3f-a586-2695f553ae7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41fe517-b1e4-4fe4-b81a-a90c3deef388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54522184-20ca-403e-b172-ff9a36987fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d362c06f-90bc-4bb9-b542-44f9707a86ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018e75b-17d0-4960-8a24-b553ffae63bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541b2b5-4c45-4eda-8760-49aabdd979a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94deb2d5-5de1-4f7f-a08f-1aa3e32ebebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a0c8fd-79bf-4109-ac81-af43cff53ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
